{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Занятие 3. Предварительная обработка данных и отбор признаков"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Веберите любые данные из репозитория данных для машинного обучения (UCI Machine learning repository: http://archive.ics.uci.edu/ml/index.php) или возьмите свои данные и проведите предварительную обработку данных и отбор признаков в соответствии со следующей схемой. Комментарии к каждому разделу обязательны."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Предварительная обработка данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rescale data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"C:/Users/79811/anaconda3/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\79811\\anaconda3\\lib\\site-packages (1.21.2)\n",
      "Requirement already satisfied: scipy in c:\\users\\79811\\anaconda3\\lib\\site-packages (1.7.1)\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.0-cp38-cp38-win_amd64.whl (7.2 MB)\n",
      "Collecting joblib>=0.11\n",
      "  Downloading joblib-1.1.0-py2.py3-none-any.whl (306 kB)\n",
      "Collecting threadpoolctl>=2.0.0\n",
      "  Downloading threadpoolctl-3.0.0-py3-none-any.whl (14 kB)\n",
      "Installing collected packages: threadpoolctl, joblib, scikit-learn\n",
      "Successfully installed joblib-1.1.0 scikit-learn-1.0 threadpoolctl-3.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy scipy scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.222 0.625 0.068 0.042]\n",
      " [0.167 0.417 0.068 0.042]\n",
      " [0.111 0.5   0.051 0.042]\n",
      " [0.083 0.458 0.085 0.042]\n",
      " [0.194 0.667 0.068 0.042]]\n"
     ]
    }
   ],
   "source": [
    "# View first 20 rows\n",
    "from pandas import read_csv\n",
    "from numpy import set_printoptions\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "filename = \"bezdekIris (2).data\"\n",
    "names = ['Длина чашелистика','Ширина чашелистика','Длина лепестка','Ширина лепестка','Класс']\n",
    "dataframe = read_csv(filename, names=names)\n",
    "array = dataframe.values\n",
    "# separate array into input and output components\n",
    "X = array[:,0:4]\n",
    "Y = array[:,4]\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "rescaledX = scaler.fit_transform(X)\n",
    "# summarize transformed data\n",
    "set_printoptions(precision=3)\n",
    "print(rescaledX[0:5,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.901  1.019 -1.34  -1.315]\n",
      " [-1.143 -0.132 -1.34  -1.315]\n",
      " [-1.385  0.328 -1.397 -1.315]\n",
      " [-1.507  0.098 -1.283 -1.315]\n",
      " [-1.022  1.249 -1.34  -1.315]]\n"
     ]
    }
   ],
   "source": [
    "# Standardize data (0 mean, 1 stdev)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from pandas import read_csv\n",
    "from numpy import set_printoptions\n",
    "filename = \"bezdekIris (2).data\"\n",
    "names = ['Длина чашелистика','Ширина чашелистика','Длина лепестка','Ширина лепестка','Класс']\n",
    "dataframe = read_csv(filename, names=names)\n",
    "array = dataframe.values\n",
    "# separate array into input and output components\n",
    "X = array[:,0:4]\n",
    "Y = array[:,4]\n",
    "scaler = StandardScaler().fit(X)\n",
    "rescaledX = scaler.transform(X)\n",
    "# summarize transformed data\n",
    "set_printoptions(precision=3)\n",
    "print(rescaledX[0:5,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.804 0.552 0.221 0.032]\n",
      " [0.828 0.507 0.237 0.034]\n",
      " [0.805 0.548 0.223 0.034]\n",
      " [0.8   0.539 0.261 0.035]\n",
      " [0.791 0.569 0.221 0.032]]\n"
     ]
    }
   ],
   "source": [
    "# Normalize data (length of 1)\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from pandas import read_csv\n",
    "from numpy import set_printoptions\n",
    "filename = \"bezdekIris (2).data\"\n",
    "names = ['Длина чашелистика','Ширина чашелистика','Длина лепестка','Ширина лепестка','Класс']\n",
    "dataframe = read_csv(filename, names=names)\n",
    "array = dataframe.values\n",
    "# separate array into input and output components\n",
    "X = array[:,0:4]\n",
    "Y = array[:,4]\n",
    "scaler = Normalizer().fit(X)\n",
    "normalizedX = scaler.transform(X)\n",
    "# summarize transformed data\n",
    "set_printoptions(precision=3)\n",
    "print(normalizedX[0:5,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binarize data (Make Binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 1. 1. 1.]\n",
      " [1. 1. 1. 1.]\n",
      " [1. 1. 1. 1.]\n",
      " [1. 1. 1. 1.]\n",
      " [1. 1. 1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "# binarization\n",
    "from sklearn.preprocessing import Binarizer\n",
    "from pandas import read_csv\n",
    "from numpy import set_printoptions\n",
    "filename = \"bezdekIris (2).data\"\n",
    "names = ['Длина чашелистика','Ширина чашелистика','Длина лепестка','Ширина лепестка','Класс']\n",
    "dataframe = read_csv(filename, names=names)\n",
    "array = dataframe.values\n",
    "# separate array into input and output components\n",
    "X = array[:,0:4]\n",
    "Y = array[:,4]\n",
    "binarizer = Binarizer(threshold=0.0).fit(X)\n",
    "binaryX = binarizer.transform(X)\n",
    "# summarize transformed data\n",
    "set_printoptions(precision=3)\n",
    "print(binaryX[0:5,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Отбор признаков"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Univariate Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 119.265   49.16  1180.161  960.007]\n",
      "[[5.1 3.5 1.4 0.2]\n",
      " [4.9 3.0 1.4 0.2]\n",
      " [4.7 3.2 1.3 0.2]\n",
      " [4.6 3.1 1.5 0.2]\n",
      " [5.0 3.6 1.4 0.2]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Feature Selection with Univariate Statistical Tests\n",
    "from pandas import read_csv\n",
    "from numpy import set_printoptions\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    "# load data\n",
    "filename = \"bezdekIris (2).data\"\n",
    "names = ['Длина чашелистика','Ширина чашелистика','Длина лепестка','Ширина лепестка','Класс']\n",
    "dataframe = read_csv(filename, names=names)\n",
    "array = dataframe.values\n",
    "X = array[:,0:4]\n",
    "Y = array[:,4]\n",
    "# feature extraction\n",
    "test = SelectKBest(score_func=f_classif, k=4)\n",
    "fit = test.fit(X, Y)\n",
    "# summarize scores\n",
    "set_printoptions(precision=3)\n",
    "print(fit.scores_)\n",
    "features = fit.transform(X)\n",
    "# summarize selected features\n",
    "print(features[0:5,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recursive Feature Elimination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Features: 2\n",
      "Selected Features: [False  True False  True]\n",
      "Feature Ranking: [3 1 2 1]\n"
     ]
    }
   ],
   "source": [
    "# Feature Selection with RFE\n",
    "from pandas import read_csv\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# load data\n",
    "filename = \"bezdekIris (2).data\"\n",
    "names = ['Длина чашелистика','Ширина чашелистика','Длина лепестка','Ширина лепестка','Класс']\n",
    "dataframe = read_csv(filename, names=names)\n",
    "array = dataframe.values\n",
    "X = array[:,0:4]\n",
    "Y = array[:,4]\n",
    "# feature extraction\n",
    "model = LogisticRegression(solver='liblinear')\n",
    "rfe = RFE(model)\n",
    "fit = rfe.fit(X, Y)\n",
    "print(\"Num Features: %d\" % fit.n_features_)\n",
    "print(\"Selected Features: %s\" % fit.support_)\n",
    "print(\"Feature Ranking: %s\" % fit.ranking_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principle Component Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained Variance: [0.925 0.053 0.017]\n",
      "[[ 0.361 -0.085  0.857  0.358]\n",
      " [ 0.657  0.73  -0.173 -0.075]\n",
      " [-0.582  0.598  0.076  0.546]]\n"
     ]
    }
   ],
   "source": [
    "# Feature Extraction with PCA\n",
    "from pandas import read_csv\n",
    "from sklearn.decomposition import PCA\n",
    "# load data\n",
    "filename = \"bezdekIris (2).data\"\n",
    "names = ['Длина чашелистика','Ширина чашелистика','Длина лепестка','Ширина лепестка','Класс']\n",
    "dataframe = read_csv(filename, names=names)\n",
    "array = dataframe.values\n",
    "X = array[:,0:4]\n",
    "Y = array[:,4]\n",
    "# feature extraction\n",
    "pca = PCA(n_components=3)\n",
    "fit = pca.fit(X)\n",
    "# summarize components\n",
    "print(\"Explained Variance: %s\" % fit.explained_variance_ratio_)\n",
    "print(fit.components_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.111 0.057 0.414 0.418]\n"
     ]
    }
   ],
   "source": [
    "# Feature Importance with Extra Trees Classifier\n",
    "from pandas import read_csv\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "# load data\n",
    "filename = \"bezdekIris (2).data\"\n",
    "names = ['Длина чашелистика','Ширина чашелистика','Длина лепестка','Ширина лепестка','Класс']\n",
    "dataframe = read_csv(filename, names=names)\n",
    "array = dataframe.values\n",
    "X = array[:,0:4]\n",
    "Y = array[:,4]\n",
    "# feature extraction\n",
    "model = ExtraTreesClassifier(n_estimators=100)\n",
    "model.fit(X, Y)\n",
    "print(model.feature_importances_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
